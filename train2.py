# -*- coding: utf-8 -*-
"""CSC240 - Housing Prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TW7m6UM-8br_0w6Xwn7WfVMrFF8eHl03
"""

import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.linear_model import Ridge
from sklearn.linear_model import Lasso
from sklearn.linear_model import LinearRegression
from sklearn.mixture import GaussianMixture
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
import xgboost as xgb
# from sklearn.preprocessing import Imputer
# from sklearn_pandas import CategoricalImputer


def pre_processing_train(train_data, test_data):
    X_train = train_data.loc[:, train_data.columns != 'SalePrice']
    X_test = test_data.loc[:, test_data.columns != 'SalePrice']

    # In[9]:

    X_combined = X_train.append(X_test, ignore_index=True)
    X_combined.shape

    # In[10]:

    def nulls(X):
        null_train = X.isnull().sum()
        null_train = null_train[null_train > 0]
        return null_train

    # In[11]:

    null_combined = nulls(X_combined)

    # In[12]:

    def dropColumns(X, nulls):
        for i in np.arange(len(nulls)):
            if nulls.values[i] > .5 * len(X):
                X = X.drop([nulls.index[i]], axis=1, inplace=False)
        return X

    # In[13]:

    X_combined = dropColumns(X_combined, null_combined)
    null_combined = nulls(X_combined)

    # In[14]:

    def impute(X, nulls):
        for i in nulls.index:
            #     print(str(data[i].dtype.name) + " " + str(i))
            #     impute_mode = Imputer(missing_values = 'NaN', strategy = 'most_frequent', axis = 0)
            #     impute_mean = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)
            if X[i].nunique() < 50:
                X[i] = X[i].fillna(X[i].mode()[0])
            else:
                X[i] = X[i].fillna(X[i].mean())
        return X
    # In[16]:

    X_combined = impute(X_combined, null_combined)
    X_combined.isnull().sum()

    # In[17]:

    def get_objectIndices(X):
        objectIndices = []
        for column in X:
            if X[column].nunique() < 50:
                objectIndices.append(X.columns.get_loc(column))
        return objectIndices

    def get_numericIndices(X):
        numericIndices = []
        for column in X:
            if X[column].nunique() >= 50:
                numericIndices.append(X.columns.get_loc(column))
        return numericIndices

    def get_numericColumnName(X):
        numericColumnName = []
        for column in X:
            if X[column].nunique() >= 50:
                numericColumnName.append(column)
        return numericColumnName
    # In[19]:

    # In[20]:
    # def remove_num_corr(X):
    #     numericColumnName = get_numericColumnName(X)
    #     numeric_combined = X.loc[:, numericColumnName]
    #     numeric_combined_corr = numeric_combined.corr()
    #     for i in numeric_combined_corr:
    #         numericCorrCount = numeric_combined_corr[i].where(lambda x: abs(x) >= .25).count()
    #         if numericCorrCount > 5:
    #             X = X.drop(i, axis=1, inplace=False)
    #     return X
    #
    # X_combined = remove_num_corr(X_combined)

    numericColumnName = get_numericColumnName(X_combined)
    scaler = preprocessing.StandardScaler()
    scaler.fit(X_combined[numericColumnName])
    X_combined[numericColumnName] = scaler.transform(X_combined[numericColumnName])
    # scaler = preprocessing.MinMaxScaler()
    # scaler.fit(X_combined[numericColumnName])
    # X_combined[numericColumnName] = scaler.transform(X_combined[numericColumnName])
    # scaler = preprocessing.RobustScaler()
    # scaler.fit(X_combined[numericColumnName])
    # X_combined[numericColumnName] = scaler.transform(X_combined[numericColumnName])

    # In[22]:

    # In[23]:

    # pca = PCA(.9)
    # pca.fit(X_combined[numericColumnName])
    # a = pca.transform(X_combined[numericColumnName])
    # a.shape
    # X_combined = X_combined.drop(numericColumnName, axis=1, inplace=False)

    objectIndices = get_objectIndices(X_combined)
    le = preprocessing.LabelEncoder()
    X_combined = X_combined.apply(le.fit_transform)
    onehotencoder = preprocessing.OneHotEncoder(categorical_features=objectIndices)
    X_combined = onehotencoder.fit_transform(X_combined).toarray()

    # X_combined = np.concatenate((X_combined, a), axis=1)

    return X_combined
    
def main():
    train_data = pd.read_csv("all/train.csv")
    validation_data = pd.read_csv("all/test.csv")

    #edited for validation
    # y_train = train_data.loc[:1300, 'SalePrice']
    y_test = train_data.loc[1300:1460, 'SalePrice']

    y_train = np.log(train_data.loc[:, 'SalePrice'])

    X_combined = pre_processing_train(train_data, validation_data)


    train_data = X_combined[:1460, :]
    validation_data = X_combined[1460:, :]

    # Testing:
    # train_data = X_combined[:1300, :]
    test_data = X_combined[1300:1460, :]

    '''
    GMM + RIDGE
    '''
    # gmm, ridges = pca_gmm_ridge_train(train_data, y_train, 4, .45)
    # y_pred_test = pca_gmm_ridge_validate(validation_data, gmm, ridges)
    #
    # y_test_pred = pca_gmm_ridge_validate(test_data, gmm, ridges)

    '''
    LINEAR REGRESSION
    '''
    # linreg = linreg_train(train_data, y_train)
    # y_pred_test = linreg_validate(linreg, validation_data)
    '''
    RANDOM FOREST
    '''
    # rf = rf_train(train_data, y_train, 1000, 25)
    # y_pred_test = rf_validate(rf, validation_data)
    '''
    GRADIENT BOOSTING
    '''
    # gb = gb_train(train_data, y_train, 2000, 0.05, 3, 10, 'sqrt', 0.75 , 'huber')
    # y_pred_test = gb_validate(gb, validation_data)
    '''
    XGBOOST
    '''
    # bst = xgboost_train(train_data, y_train, 0.4603, 0.0468, 0.05, 3, 1.7817, 2200, 0.4640, 0.8571, 0.5213, 1, 7, -1)
    # y_pred_test = xgboost_validate(bst, validation_data)
    '''
    LASSO REGRESSION
    '''
    # lasso = lasso_train(train_data, y_train, .001)
    # y_pred_test = lasso_validate(lasso, validation_data)
    '''
    RIDGE REGRESSION
    '''
    ridge = ridge_train(train_data, y_train, .45)
    y_pred_test = ridge_validate(ridge, validation_data)

    '''
    STACKING WITH RIDGE, GRADIENT BOOSTING AND XGBOOST
    '''
    # ridge = ridge_train(train_data, y_train, .45)
    # ridge_test = ridge_validate(ridge, validation_data)
    # gb = gb_train(train_data, y_train, 2000, 0.05, 3, 10, 'sqrt', 0.75 , 'huber')
    # gb_test = gb_validate(gb, validation_data)
    # bst = xgboost_train(train_data, y_train, 0.4603, 0.0468, 0.05, 3, 1.7817, 2200, 0.4640, 0.8571, 0.5213, 1, 7, -1)
    # bst_test = xgboost_validate(bst, validation_data)
    # y_pred_test = (0.3355872739*np.exp(ridge_test)+0.3342114825*np.exp(gb_test)+0.3302012436*np.exp(bst_test))


    '''
    END OF MODEL PREDICTIONS
    '''
    # y_pred = np.array(y_pred, dtype=float)
    # y_test_pred = np.array(y_test_pred, dtype=float)
    #
    # y_pred = [abs(y) if y < 0 else y for y in y_pred]
    # y_test_pred = [abs(y) if y < 0 else y for y in y_test_pred]

    # preds = zip(np.exp(y_pred_test), np.exp(y_train))
    preds = zip(np.exp(y_pred_test), np.exp(y_train))

    for p in preds:
        print(p)


    # print("RMSLE = ", rmsle(y_pred_test, y_train))

    df_out = pd.DataFrame()
    df_out["SalePrice"] = np.exp(y_pred_test)
    # df_out["SalePrice"] = y_pred_test
    df_out["Id"] = df_out.index + 1461
    df_out.to_csv("predictions.csv", header=["SalePrice", "Id"], index=False)



def rmsle(y_pred, y_actual):
    return mean_squared_error(np.log(y_pred), np.log(y_actual))


def pca_gmm_ridge_train(train_data, y_train, num_clusters, _alpha):
    '''
    :param train_data: file path to the training data
    :return: PCA, GMM, Ridge Regression Models
    '''
    # Testing alphas, 1 looks fine
    # for a in np.arange(0, 5.0, .1):
    #     ridge = Ridge(alpha = a)
    #     ridge.fit(X_train, y_train)
    #     r2 = ridge.score(X_train, y_train)
    #     print("Final R2=", r2, "Final alpha = ", a)

    # a = 1
    # ridge = Ridge(alpha=a)
    # ridge.fit(X_train, y_train)
    # r2 = ridge.score(X_train, y_train)
    # print("Final R2=", r2, "Final alpha = ", a)

    gmm = GaussianMixture(num_clusters)
    gmm.fit(train_data, y_train)
    # print(gmm.means_)
    # print(gmm.weights_)

    #initialize train sets for each cluster
    X_cluster_train = [np.ndarray(shape=(0, len(train_data[0]))) for i in range(num_clusters)]
    Y_cluster_train = [np.ndarray(shape=(0)) for i in range(num_clusters)]

    gmm_predictions = gmm.predict(train_data)
    # print(gmm_predictions)

    for i in range(len(train_data)):
        pred = gmm_predictions[i]
        X_cluster_train[pred] = np.append(X_cluster_train[pred], train_data[i].reshape(1, len(train_data[i])), axis = 0)
        Y_cluster_train[pred] = np.append(Y_cluster_train[pred], y_train[i])

    # print(X_cluster_train)

    # for c in X_cluster_train:
    #     print(len(c))
    # for c in Y_cluster_train:
    #     print(len(c))
    # print(len(X_train))

    linregs = [None for i in range(num_clusters)]
    for c in range(len(X_cluster_train)):
        ridge = Ridge(alpha=_alpha)
        ridge.fit(X_cluster_train[c],Y_cluster_train[c])
        print("R2 =", ridge.score(X_cluster_train[c], Y_cluster_train[c]))
        linregs[c] = ridge

    print(train_data.shape)
    # return (pca, gmm, ridge)
    return (gmm, linregs)

def pca_gmm_ridge_validate(test_data, gmm, ridge):
    '''
    :param test_data: the data to run the model on
    :param pca: the PCA spec generated from training
    :param gmm: GMM spec from training
    :param ridge: ridge regressions from training
    :return:
    '''

    y_pred = np.ndarray(shape=(0))

    gmm_predictions = gmm.predict_proba(test_data)

    for i in range(len(test_data)):
        pred = 0
        for j in range(len(gmm_predictions[i])):
            pred += ridge[j].predict(test_data[i].reshape(1, -1)) * gmm_predictions[i, j]
        y_pred = np.append(y_pred, pred)

    return y_pred


def rf_train(train_data, y_train, _estimators, _min_samples):
    rf = RandomForestRegressor(n_estimators=_estimators, min_samples_leaf=_min_samples)
    rf.fit(train_data, y_train)
    return rf

def rf_validate(rf, test_data):
    return rf.predict(test_data)


def gb_train(train_data, y_train, _estimators, _learning_rate, _max_depth, _min_samples_split, _max_features, _subsample, _loss):
    # gb = GradientBoostingRegressor(min_samples_leaf = _min_samples, n_estimators = _estimators)
    gb = GradientBoostingRegressor(n_estimators = _estimators, learning_rate = _learning_rate, max_depth = _max_depth, min_samples_split=_min_samples_split, max_features=_max_features, subsample=_subsample, loss=_loss)
    gb.fit(train_data, y_train)
    # gb = gb_train(n_estimators=2000, learning_rate=0.05, max_depth=3, min_samples_split=10, max_features='sqrt',
    #               subsample=0.75, loss='huber')
    return gb

def gb_validate(gb, test_data):
    return gb.predict(test_data)

def ridge_train(train_data, y_train, _alpha):
    ridge = Ridge(alpha=_alpha)
    ridge.fit(train_data, y_train)
    return ridge
def ridge_validate(ridge, test_data):
    return ridge.predict(test_data)

def lasso_train(train_data, y_train, _alpha):
    lasso = Lasso(alpha=_alpha)
    lasso.fit(train_data, y_train)
    return lasso
def lasso_validate(lasso, test_data):
    return lasso.predict(test_data)

def linreg_train(train_data, y_train):
    linreg = LinearRegression()
    linreg.fit(train_data, y_train)
    return linreg
def linreg_validate(linreg, test_data):
    return linreg.predict(test_data)
def xgboost_train(train_data, y_train, colsample_bytree, gamma, learning_rate, max_depth, min_child_weight, n_estimators, reg_alpha, reg_lambda, subsample, silent, random_state, nthread):
    bst = xgb.XGBRegressor(colsample_bytree=colsample_bytree, gamma=gamma,
                             learning_rate=learning_rate, max_depth=max_depth,
                             min_child_weight=min_child_weight, n_estimators=n_estimators,
                             reg_alpha=reg_alpha, reg_lambda=reg_lambda,
                             subsample=subsample, silent=silent,
                             random_state =random_state, nthread = nthread)
    bst.fit(train_data, y_train)
    return bst
def xgboost_validate(bst, test_data):
    return bst.predict(test_data)

if __name__ == "__main__":
    main()